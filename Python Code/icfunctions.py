# -*- coding: utf-8 -*-
"""icfunctions.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1RXc2A243yRO7LMUhbaVNERVbTxjJCxWk

## Funções de Manejo de Dados
"""



"""### Função para achar os picos e cortar o dado em contração/base line"""

def data_division(data = [], peaks_height = 0.05, peaks_dist = 4000,
                  contraction_division_window = 2500, baseline_overlap = 300,
                  plot = False):
  '''
  Parameters:
  data: List or array of signal data
  peaks_height: Minimum height of the peaks for detection
  peaks_dist: Minimum distance between peaks
  contraction_division_window: Time window to divide the contraction
  baseline_overlap: Baseline overlap
  plot: Boolean to indicate whether to plot the results (default is False)

  Return:
  - peaks: Peaks of the signal
  - baseline: All baseline parts with zeros where it is not the baseline
  - contractions: All contraction parts with zeros where it is not the contraction
  - vector_baseline: A matrix of vectors where each vector is one baseline
  - normalized_baseline_vector: A matrix of vectors where each vector is one baseline with the same size as twice the contraction window
  - vector_contractions: A matrix of vectors where each vector is one contraction
  '''

  import pandas as pd
  import numpy as np
  from scipy.signal import find_peaks
  import matplotlib.pyplot as plt

  #----------- Finding the data peaks ------------
  peaks,_ = find_peaks(data, height = peaks_height, distance = peaks_dist)

  #----------- Cuttiing the contraction and the base line windows ------------

  #Creatting contraction vectors with the same size than the original signal
  contractions = np.zeros_like(data)
  vector_contractions = []
  for i in peaks:
    contractions[i-contraction_division_window:i + contraction_division_window] = data[i-contraction_division_window:i + contraction_division_window]
    vector_contractions.append(data[i-contraction_division_window:i + contraction_division_window])
  #Cutting the base lines
  auxiliar_vector = np.zeros_like(data)#this auxiliar vector is going to store the contractions we want to remove with an overlap
  for i in peaks:
    auxiliar_vector[i - (contraction_division_window + baseline_overlap) : i + (contraction_division_window + baseline_overlap)] = data[i - (contraction_division_window + baseline_overlap) : i + (contraction_division_window + baseline_overlap)]
  baseline = data - auxiliar_vector #the vector which stores all the base lines together with zeros on contraction
  #Creatting the matrix with each base line vector
  vector_baseline = []
  aux = 0
  for i in peaks:
    vector_baseline.append(baseline[aux : i - (contraction_division_window + baseline_overlap)])
    aux = i + (contraction_division_window + baseline_overlap)
    if(i == peaks[len(peaks)-1]):
      #if we are at the last peak, we have to catch all the data starting on i + (contraction_division_window + baseline_overlap) and going until the end
      vector_baseline.append(baseline[i + (contraction_division_window + baseline_overlap) : ])

  #----------- Normalizing the length of all vectors accordding to the contractrion_division_window------------

  normalized_baseline_vector = []
  for i in vector_baseline:
    #If the baseline is shorter than twice the division window, we are going to mean pad
    if(len(i) <= contraction_division_window*2):
      tamanho = int(contraction_division_window*2 - len(i))
      aux = np.pad(i,(0,tamanho), mode ='constant',constant_values=np.mean(i))
      normalized_baseline_vector.append(aux)
    else:
      inicio = len(i)//2 - contraction_division_window
      fim =  len(i)//2 + contraction_division_window
      normalized_baseline_vector.append(i[inicio:fim])
      """
      normalized_baseline_vector.append(i[0:contraction_division_window*2])
      """
  #----------- If the user choose to plot, whe plot the base line and the contractions ------------
  if(plot):
    #Plotting the result
    plt.figure( figsize=(15,8))
    plt.title('Contractions on your data')
    plt.plot(data, label = 'original signal')
    plt.plot(contractions,label='contractions')
    plt.plot(peaks,data[peaks], 'x', label='picos')
    plt.xticks(np.arange(0,len(data),1000))
    plt.legend(loc="upper right")
    plt.grid()
    plt.tight_layout()

    plt.figure(figsize=(15,8))
    plt.tight_layout()
    plt.title('Base Line on your data')
    plt.plot(data, label = 'original signal')
    plt.plot(baseline,label='base line')
    plt.plot(peaks,data[peaks], 'x', label='picos')
    plt.axhline(y=np.max(baseline), color='r', linestyle='--', label = 'threshold')
    plt.xticks(np.arange(0,len(data),1000))
    plt.legend(loc="upper right")
    plt.grid()
    plt.tight_layout()


  return (peaks, baseline, contractions, vector_baseline, normalized_baseline_vector, vector_contractions)

"""### Função para embaralhar o sinal"""

def shuffering(x, y, times = None):
  '''
  This function receives X and y vector and shuffer it randomly if
  times was not provided.

  Parameters:
  - x (numpy array): The first vector to be shuffled.
  - y (numpy array): The second vector to be shuffled.
  - times (int, optional): The number of times to shuffle the vectors.

  Returns:
  - shuffled[0] (numpy array): The shuffled first vector.
  - shuffled[1] (numpy array): The shuffled second vector.
  - sequencia_i (numpy array): The sequence of random_state parameters used.

  '''

  import numpy as np
  import pandas as pd
  from sklearn.utils import shuffle

  if times == None:
    n = np.random.randint(0,43)
  else:
    n = times

  shuffled = [x,y]
  sequencia_i = []
  for i in range(n):
    shuffled = shuffle(shuffled[0],shuffled[1],random_state = i)
    sequencia_i.append(i)

  print(f'The data was shuffled {sequencia_i[-1]} times with the last random state parameter being also {sequencia_i[-1]}')

  return shuffled[0], shuffled[1], sequencia_i

"""### Função para plotar o gráfico dos dados no domínio do tempo com o devido vetor tempo"""

def plotar_tempo(dados = [], fs = 1000, nome = None):

  '''
  This function plots the provided data in the time domain.

  Parameters:
  dados: List or array of signal data
  fs: Sampling frequency (default is 1000 Hz)
  nome: Name or description of the data being plotted (default is None)

  Return:
  None. The function plots the data and does not return any value.
  '''

  import matplotlib.pyplot as plt
  import numpy as np

  #Obtendo o valor do período
  T = 1/fs
  #Criando o vetor tempo
  t = np.arange(len(dados)) * T

  #Plotando no domínio do Tempo
  plt.figure(figsize=(10, 3))
  plt.plot(t,dados)
  plt.title(f'Amplitude x tempo de {nome}')
  plt.xlabel('Tempo (s)')
  plt.ylabel('Amplitude')
  #plt.xlim(0,max(t)+10)
  plt.xticks(np.arange(0,max(t),1))
  plt.grid()
  plt.show()

"""### Função para transformar os dados para domínio da frequência"""

def dominio_freq(fs=1000, dados =[],plotar = True, name= None):

  '''
  This function returns an array containing the normalized magnitude and frequency vector of the input data,
  and plots them if desired.

  Parameters:
  fs: Sampling frequency (1/s). Default: 1000
  dados: Vector to be transformed to the frequency domain
  plotar: Boolean indicating whether to plot the result in the frequency domain. Default: True
  name: Name of the data being plotted. Default: 'Coloque aqui o que você está plotando'

  Returns:
  - vetor_freq: A list where vetor_freq[0] is the frequency vector and vetor_freq[1] is the magnitude vector
  - Plots the graph if plotar is True
  '''

  #----- Importando as bibliotecas -----
  import numpy as np
  import matplotlib.pyplot as plt


  #----- Cálculo na frequência -----
  #Período de Amostragem
  T = 1/fs
  # Transformada Rápida de Fourier
  dfft = np.fft.fft(dados)
  #vetor frequência
  freq = np.fft.fftfreq(len(dados), T)
  #magnitude
  mag = abs(dfft)
  #normalização da magnitude
  normag = mag/mag.size

  vetor_freq = []
  vetor_freq.append(freq)
  vetor_freq.append(normag)

  #----- visualização na frequência -----
  #plotando o sinal no domínio da frequência
  if plotar:
    plt.figure()
    plt.figure(figsize= (10,3))
    plt.plot(freq, mag)
    plt.title(f'Magnitude x Frequência do {name}')
    plt.xlabel('Frequência (Hz)')
    plt.ylabel('Magnitude')
    plt.xticks(np.arange(0,max(freq),30))
    plt.grid()
  else: print('Como plotar = falso, não foi plotado o gráfico na frequência dos dados inseridos')

  return vetor_freq

"""### Função para ver o espectograma"""

def espectograma(fs = 1000, dados = [], name= None) :
  '''
  This function plots the spectrogram of the provided data.

  Parameters:
  fs: Sampling frequency (1/s). Default: 1000
  dados: Array of signal data
  name: Name of the data being plotted. Default: None

  Returns:
  - Plots the spectrogram of the provided data
  '''

  import numpy as np
  import matplotlib.pyplot as plt

  resolucao = int(fs/10)
  plt.figure(figsize=(10, 3))
  plt.specgram(dados, Fs=fs, NFFT = 226)
  plt.colorbar()
  plt.title(f'Espectrograma de {name}')
  plt.xlabel('Tempo')
  plt.ylabel('Frequência')
  plt.grid()
  plt.show()

"""### Função para achar a densidade espectral do sinal"""

def psd(data = [], Fs = 603, plot = False, log = True, title = None):

  '''
  This function calculates and optionally plots the Power Spectral Density (PSD) of the provided data.

  Parameters:
  data: List or array of signal data
  Fs: Sampling frequency (Hz). Default: 603
  plot: Boolean to indicate whether to plot the results. Default: False
  log: Boolean to indicate whether to plot in logarithmic scale. Default: True
  title: Title of the plot. Default: None

  Returns:
  - Psd: Power Spectral Density of the data
  '''

  import numpy as np
  from scipy import signal
  import matplotlib.pyplot as plt

  fs = 1000
  f, Psd = signal.welch(data, fs = Fs, nperseg = 4*256,noverlap= int(4*256 * 0.1),  nfft = len(data), )
  Psd_db = 20 *np.log(Psd)
  if(plot):
    if(log):
      plt.figure()
      plt.title(f'Power Spectral Density of {title}')
      plt.xlabel('Frequency (Hz)')
      plt.ylabel('Power/Frequency (db/Hz)')
      plt.plot(f, Psd_db, color = 'b', label = 'PSD')
      plt.legend(loc="upper right")
      plt.grid()
      plt.tight_layout()
    else:
      plt.figure()
      plt.title(f'Power Spectral Density of {title}')
      plt.xlabel('Frequency (Hz)')
      plt.ylabel('Power/Frequency (unity/Hz)')
      plt.plot(f, Psd,color = 'b', label = 'PSD')
      plt.legend(loc="upper right")
      plt.grid()
      plt.tight_layout()
  return(Psd)

"""## Funções para os Modelos de Machine Learning e para avaliá-los

### PCA
"""

def apply_pca(X=[],y=[], X_test=[], y_test = [], plot = True,title=None):

  '''
  This function applies PCA to the data, plots the results if desired, and classifies the provided test data based on Euclidian Distance of the centroid.

  Parameters:
  X: Training set
  y: Training labels
  X_test: Test set
  y_test: Test labels
  plot: Boolean to indicate whether to plot the results (default is True)
  title: Title for the plot (default is None)

  Returns:
  - X_train_pca: The transformed training set using PCA components
  - centroids: The centroids of each class
  - y_score: The highest probability score for each test sample
  - y_class_score: A matrix with the probability of each test sample belonging to each class
  '''

  import numpy as np
  import pandas as pd
  import matplotlib.pyplot as plt
  import time
  from sklearn.decomposition import PCA
  from scipy.spatial.distance import cdist
  from scipy.special import softmax

  # Initialize PCA model
  start_time = time.time() #start time for training
  pca = PCA(n_components=2)  # Adjust n_components

  # Fit PCA model to the data
  X_train_pca = pca.fit_transform(X)
  end_time = time.time() #end time for training
  print(f'Training time: {(end_time - start_time)*1000:.4f} ms')

  # Transforming the test data
  start_time = time.time() #start time for testing
  X_test_pca = pca.transform(X_test)

  #Calculating the centroids in the reduced PCA space
  centroids = []
  for class_label in np.unique(y):
      centroid = X_train_pca[y == class_label].mean(axis=0)
      centroids.append(centroid)
  centroids = np.array(centroids)

  #Predicting the values
  distances = cdist(X_test_pca, centroids, 'euclidean')
  # Convert distances to similarity scores (the smaller the distance, the higher the similarity)
  similarity_scores = 1/(distances + 1e-6) # I added a small value to avoid division by zero
  # Apply softmax to similarity scores to get probabilities
  y_score = softmax(similarity_scores, axis=1)
  y_class_score = np.argmax(y_score, axis=1)
  end_time = time.time() #end time for testing
  print(f'Testing/prediction time: {(end_time - start_time)*1000:.4f} ms')

  #Plotting the results
  if plot:
    plt.figure(figsize=(10, 6))
    colors_train = ['deepskyblue', 'green', 'red']
    colors_centroid = ['blue', 'greenyellow','fuchsia']
    colors_test = ['cornflowerblue', 'lime', 'orange']

    # Plot training data
    for i, color in enumerate(colors_train):
        plt.scatter(X_train_pca[y == i, 0], X_train_pca[y == i, 1], c=color, marker='o', edgecolor='black', label=f'Class {i} (Train)',alpha=0.65)

    # Plot test data
    for i, color in enumerate(colors_test):
        plt.scatter(X_test_pca[y_class_score == i, 0], X_test_pca[y_class_score == i, 1], c=color, marker='^', edgecolor='black', label=f'Classified as class {i}')

    # Plot centroids
    for i, color in enumerate(colors_centroid):
        plt.scatter(centroids[i, 0], centroids[i, 1], c=color, marker='X', s=150, edgecolor='black', label=f'Centroid Class {i}')

    plt.xlabel('PC1 Component')
    plt.ylabel('PC2 Component')
    plt.title(f'PCA Classification with Euclidean Distance for {title}')
    plt.legend()
    plt.grid()
    plt.show()

  return X_train_pca, centroids, y_score, y_class_score

"""### LDA"""

def apply_lda(X=[],y=[], X_test=[], y_test = [], plot = True,title=None):

  '''
  This function fits the data to the LDA model, optionally plots the results, and classifies
  the provided test data.
  Note: Data must be split and standardized.

  Parameters:
  X: Training set
  y: Training classes
  X_test: Test set
  y_test: Test classes
  plot: Boolean to indicate whether to plot the results (default is True)
  title: Title for the plot (default is None)

  Returns:
  - lda_result: Result of the fitting between X (train) and y (train)
  - X_lda: The transformation of X by the LDA components
  - y_score: The value of predict_proba(), i.e., an array with the probability of each signal belonging to each class
  - y_class_score: The value of predict(), i.e., an array with the classification of each signal into each class
  '''

  import pandas as pd
  import numpy as np
  import matplotlib.pyplot as plt
  import time
  from sklearn.discriminant_analysis import LinearDiscriminantAnalysis
  from sklearn.metrics import accuracy_score

  # Initialize LDA model
  start_time = time.time() #start time for training
  lda = LinearDiscriminantAnalysis(n_components=2)

  # Fit LDA model to the data
  lda_result = lda.fit(X, y)

  # Transform the data using the LDA model
  X_lda = lda.transform(X)
  end_time = time.time() #end time for training
  print(f'Training time: {(end_time - start_time)*1000:.4f} ms')
  start_time = time.time() #start time for testing
  X_test_transform = lda.transform(X_test)

  # Classifying the test data
  y_score = lda.predict_proba(X_test)
  y_class_score = lda.predict(X_test)
  end_time = time.time() #end time for testing
  print(f'Testing/prediction time: {(end_time - start_time)*1000:.4f} ms')

  # Accuracy Evaluation of the model
  accuracy = accuracy_score(y_test, y_class_score)
  print(f'Accuracy: {accuracy * 100:.2f}%')

  # If wanted, visualize the LDA results
  if plot:
    plt.figure(figsize=(10, 6))
    colors_train = ['blue', 'green', 'red']
    colors_test = ['cornflowerblue', 'lime', 'orange']

    for label, color in enumerate(colors_train):
      plt.scatter(X_lda[y==label, 0],X_lda[y==label, 1], c=color, marker='o', edgecolor='black', label=f'Class {label} (Train)', alpha =.8)
    for label, color in enumerate(colors_test):
      plt.scatter(X_test_transform[y_class_score==label, 0],X_test_transform[y_class_score==label, 1],
                   c=color, marker='^', edgecolor='black', label=f'Classified as class {label}')
    plt.xlabel('LD1')
    plt.ylabel('LD2')
    plt.title(f'{title}')
    plt.legend()
    plt.grid()
    plt.show()

  return lda_result, X_lda, y_score, y_class_score

"""### QDA"""

def apply_qda(X=[],y=[], X_test=[], y_test = [], plot = True,title=None):

  '''
  This function fits the data to the QDA model, optionally plots the results, and classifies
  the provided test data.
  Note: Data must be split and standardized.

  Parameters:
  X: Training set
  y: Training classes
  X_test: Test set
  y_test: Test classes
  plot: Boolean to indicate whether to plot the results (default is True)
  title: Title for the plot (default is None)

  Returns:
  - qda_result: Result of the fitting between X (train) and y (train)
  - X_lda: The transformation of X by the LDA components
  - y_score: The value of predict_proba(), i.e., an array with the probability of each signal belonging to each class
  - y_class_score: The value of predict(), i.e., an array with the classification of each signal into each class
  '''

  import numpy as np
  import pandas as pd
  import matplotlib.pyplot as plt
  import time
  from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis, LinearDiscriminantAnalysis
  from sklearn.metrics import accuracy_score


  # Initialize and train the QDA model
  start_time = time.time() #start time for training
  qda = QuadraticDiscriminantAnalysis()
  qda_result = qda.fit(X, y)
  end_time = time.time() #end time for training
  print(f'Training time: {(end_time - start_time)*1000:.4f} ms')

  # Predict probabilities on the test set
  start_time = time.time() #start time for testing
  y_score = qda.predict_proba(X_test)
  y_class_score = qda.predict(X_test)
  end_time = time.time() #end time for testing
  print(f'Testing/prediction time: {(end_time - start_time)*1000:.4f} ms')

  # Using LDA to reduct the dimension and plot the data with LDA components
  lda = LinearDiscriminantAnalysis(n_components=2)
  X_lda = lda.fit_transform(X, y)
  X_test_lda_transform = lda.transform(X_test)

  # Accuracy Evaluation of the model
  accuracy = accuracy_score(y_test, y_class_score)
  print(f'Accuracy: {accuracy * 100:.2f}%')

  # If wanted, visualize the QDA results
  if plot:
    plt.figure(figsize=(10, 6))
    colors_train = ['blue', 'green', 'red']
    colors_test = ['cornflowerblue', 'lime', 'orange']

    for label, color in enumerate(colors_train):
      plt.scatter(X_lda[y==label, 0],X_lda[y==label, 1], c=color, marker='o', edgecolor='black', label=f'Class {label} (Train)', alpha =.8)
    for label, color in enumerate(colors_test):
      plt.scatter(X_test_lda_transform[y_class_score==label, 0],X_test_lda_transform[y_class_score==label, 1],
                   c=color, marker='^', edgecolor='black', label=f'Classified as class {label}')

    plt.xlabel('LD1')
    plt.ylabel('LD2')
    plt.title(f'{title}')
    plt.legend()
    plt.grid()
    plt.show()

  return qda_result, X_lda, y_score, y_class_score

"""### PLS"""

def apply_pls(X=[],y=[], X_test=[], y_test = [], plot = True,title=None):

  '''
  This function fits the data to the PLS model, optionally plots the results, and classifies
  the provided test data.

  Parameters:
  X: Training set
  y: Training classes
  X_test: Test set
  y_test: Test classes
  plot: Boolean to indicate whether to plot the results (default is True)
  title: Title for the plot (default is None)

  Returns:
  - pls_result: Result of the fitting between X (train) and y (train)
  - X_train_pls: The transformation of X by the PLS components
  - y_score: The binarized value of decision_function(), i.e., an array with the distance to the hyperplane
  - y_class_score: The value of predict(), i.e., an array with the classification of each signal into each class
  '''

  import numpy as np
  import pandas as pd
  import matplotlib.pyplot as plt
  import time
  from sklearn.cross_decomposition import PLSRegression
  from sklearn.metrics import accuracy_score
  from sklearn.preprocessing import label_binarize

  # Initialize PLS model
  start_time = time.time() #start time for training
  pls = PLSRegression(n_components=2)  # Adjust n_components

  #Fitting the model
  pls_result =pls.fit(X,y)

  # Transform the data to PLS components
  X_train_pls = pls.transform(X)
  end_time = time.time() #end time for training
  print(f'Training time: {(end_time - start_time)*1000:.4f} ms')

  start_time = time.time() #start time for testing
  X_test_pls = pls.transform(X_test)

  # Predict probabilities on the test set
  y_class_score = pls.predict(X_test)

  #Making the y_score array
  y_score = y_class_score.tolist()
  for i, valor in enumerate(y_score):
    y_score[i] = round(abs(valor))
  y_score = np.array(y_score)
  y_score = label_binarize(y_score, classes=[0, 1, 2])

  end_time = time.time() #end time for testing
  print(f'Testing/prediction time: {(end_time - start_time)*1000:.4f} ms')

  if plot:
    # Visualization of PLS components
    plt.figure(figsize=(10, 6))
    #Plotting the results
    colors_train = ['blue', 'green', 'red']
    colors_test = ['cornflowerblue', 'lime', 'orange']

    for label, color in enumerate(colors_train):
        plt.scatter(X_train_pls[y == label, 0], X_train_pls[y == label, 1], c=color, marker='o', edgecolor='black', label=f'Class {label} (Train)', alpha =.8)
    for label , color in enumerate(colors_test):
        plt.scatter(X_test_pls[y_test == label, 0], X_test_pls[y_test == label, 1],
                    c=color, marker='^', edgecolor='black', label=f'Classified as class {label}')
    plt.xlabel('PLS Component 1')
    plt.ylabel('PLS Component 2')
    plt.title(title)
    plt.legend()
    plt.grid()
    plt.show()

  return pls_result, X_train_pls, y_score, y_class_score

"""### SVM

"""

def apply_svm(X, y, X_test, y_test, plot=True, title=None):

  '''
  This function fits the data to the SVM model, optionally plots the results, and classifies
  the provided test data.

  Parameters:
  X: Training set
  y: Training classes
  X_test: Test set
  y_test: Test classes
  plot: Boolean to indicate whether to plot the results (default is True)
  title: Title for the plot (default is None)

  Returns:
  - svm_result: Result of the fitting between X (train) and y (train)
  - X_pca: The transformation of X, i.e., the new plane made by PC1 and PC2
  - y_score: The value of decision_function(), i.e., an array with the distance to the hyperplane
  - y_class_score: The value of predict(), i.e., an array with the classification of each signal into each class
  '''

  import pandas as pd
  import numpy as np
  import matplotlib.pyplot as plt
  import time
  from sklearn.svm import SVC
  from sklearn.decomposition import PCA
  from sklearn.metrics import accuracy_score

  # Inicializar o modelo SVM
  start_time = time.time() #start time for training
  svm = SVC(probability=True)  # Ajuste os parâmetros conforme necessário

  # Ajustar o modelo SVM aos dados
  svm_result = svm.fit(X, y)
  end_time = time.time() #end time for training
  print(f'Training time: {(end_time - start_time)*1000:.4f} ms')

  # Prever as probabilidades no conjunto de teste
  start_time = time.time() #start time for testing
  y_score = svm.predict_proba(X_test)

  # Prever as classes no conjunto de teste
  y_class_score = svm.predict(X_test)
  end_time = time.time() #end time for testing
  print(f'Testing/prediction time: {(end_time - start_time)*1000:.4f} ms')

  # Usar PCA para reduzir a dimensionalidade para 2D para visualização
  pca = PCA(n_components=2)
  X_pca = pca.fit_transform(X)
  X_test_pca = pca.transform(X_test)

  # Accuracy Evaluation of the model
  accuracy = accuracy_score(y_test, y_class_score)
  print(f'Accuracy: {accuracy * 100:.2f}%')

  # Se desejar, visualizar os resultados do SVM
  if plot:
    plt.figure(figsize=(10, 6))
    colors_train = ['blue', 'green', 'red']
    colors_test = ['cornflowerblue', 'lime', 'orange']

    for label, color in enumerate(colors_train):
        plt.scatter(X_pca[y == label, 0], X_pca[y == label, 1], c=color, marker='o', edgecolor='black', label=f'Class {label} (Train)', alpha =.8)
    for label, color in enumerate(colors_test):
        plt.scatter(X_test_pca[y_class_score == label, 0], X_test_pca[y_class_score == label, 1],
                    c=color, marker='^', edgecolor='black', label=f'Classified as class {label}')
    plt.xlabel('PC1')
    plt.ylabel('PC2')
    plt.title(title)
    plt.grid()
    plt.legend()
    plt.show()

  return svm_result, X_pca, y_score, y_class_score

"""### ANN"""

def build_and_apply_ann(X, y, X_test, y_test, epochs=50, batch_size=32,
                        plot=True, title=None):
  '''
  Function to create, train, and apply a neural network as a classifier.

  Parameters:
  X_train: Training set
  y_train: Training classes
  X_test: Test set
  y_test: Test classes
  epochs: Number of epochs
  batch_size: Batch size

  Returns:
  model: Neural network model
  history: Training history
  y_score: Probability of each class
  y_class_score: Classification of each class
  '''

  import tensorflow as tf
  import numpy as np
  import matplotlib.pyplot as plt
  import time
  from sklearn.discriminant_analysis import LinearDiscriminantAnalysis
  from sklearn.decomposition import PCA

  #Creating ANN
  start_time = time.time() #start time for training
  model = tf.keras.Sequential()
  model.add(tf.keras.layers.Dense(64, activation='relu', input_shape=(X.shape[1],)))
  model.add(tf.keras.layers.Dense(32, activation='relu'))
  model.add(tf.keras.layers.Dense(3, activation='softmax'))

  #Compilando a rede neural
  model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),
                loss='sparse_categorical_crossentropy',
                metrics='accuracy')

  #Training the model
  history = model.fit(X, y, epochs=epochs, batch_size=batch_size,verbose=0)
  end_time = time.time() #end time for training
  print(f'Training time: {(end_time - start_time)*1000:.4f} ms')

  #Testing the model
  #Classifying the data provided
  start_time = time.time() #start time for testing
  y_score = model.predict(X_test) #probabilidade de ser de cada classe
  y_class_score = np.argmax(y_score, axis=1) #qual classe ele é
  end_time = time.time() #end time for testing
  print(f'Testing/prediction time: {(end_time - start_time)*1000:.4f} ms')

  if plot:
    # Using PCA to reduction and visualize the data
    pca = PCA(n_components=2)
    X_train_pca = pca.fit_transform(X)
    X_test_pca = pca.transform(X_test)

    # Visualizar os resultados do ANN
    plt.figure(figsize=(10, 6))
    colors_train = ['blue', 'green', 'red']
    colors_test = ['cornflowerblue', 'lime', 'orange']
    for label, color in enumerate(colors_train):
        plt.scatter(X_train_pca[y == label, 0], X_train_pca[y == label, 1], c=color, marker='o', edgecolor='black', label=f'Class {label} (Train)', alpha =.8)
    for label, color in enumerate(colors_test):
        plt.scatter(X_test_pca[y_class_score == label, 0], X_test_pca[y_class_score == label, 1],
                   c=color, marker='^', edgecolor='black', label=f'Classified as class {label}')
    plt.title(f'{title}')
    plt.xlabel('PC1')
    plt.ylabel('PC2')
    plt.grid()
    plt.legend()
    plt.show()

    return model, y_score, y_class_score, history

"""### ROC Curve"""

def plot_ROC(probs= [],y_test = [],title= None, y_binarized= False ):

  '''
  Function to plot ROC curves for multiclass classification.

  Parameters:
  probs: Predicted probabilities for each class
  y_test: True labels for the test set
  title: Title for the plot
  y_binarized: Boolean indicating if y_test should be binarized (default is False)

  Returns:
  None (plots the ROC curves)
  '''

  import matplotlib.pyplot as plt
  from sklearn.metrics import roc_curve, auc
  from sklearn.preprocessing import label_binarize

  #Binaryze y_test
  if not(y_binarized):
    y_test_binarized = label_binarize(y_test, classes=[0, 1, 2])
  else:
    y_test_binarized = y_test

  # Calcular a curva ROC e a área ROC para cada classe
  fpr = dict()
  tpr = dict()
  roc_auc = dict()
  n_classes = y_test_binarized.shape[1]

  for i in range(n_classes):
      fpr[i], tpr[i], _ = roc_curve(y_test_binarized[:, i], probs[:, i])
      roc_auc[i] = auc(fpr[i], tpr[i])

  # Plotar a curva ROC para cada classe
  plt.figure()
  colors = ['aqua', 'darkorange', 'cornflowerblue'] #change color
  for i, color in zip(range(n_classes), colors):
      plt.plot(fpr[i], tpr[i], color=color, lw=2,
              label=f'Class {i} ROC curve (area = {roc_auc[i]:.2f})')

  # Plotar linha de predição aleatória
  plt.plot([0, 1], [0, 1], 'k--', lw=2, label='Random prediction (AUC = 0.50)')
  plt.xlim([0.0, 1.0])
  plt.ylim([0.0, 1.05])
  plt.xlabel('False Positive Rate')
  plt.ylabel('True Positive Rate')
  plt.title(title)
  plt.legend(loc="lower right")
  plt.grid()
  plt.show()

"""### Classification Report"""

def metrics(y_test, y_class_score):
  '''
  Function to print classification metrics using sklearn's classification_report.

  Parameters:
  y_test: True labels for the test set
  y_class_score: Predicted labels for the test set

  Returns:
  None (prints the classification report)
  '''

  from sklearn.metrics import classification_report

  print(classification_report(y_test, y_class_score))



